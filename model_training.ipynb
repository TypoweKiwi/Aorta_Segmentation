{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11209360,"sourceType":"datasetVersion","datasetId":6999385},{"sourceId":12670650,"sourceType":"datasetVersion","datasetId":8007097},{"sourceId":254146075,"sourceType":"kernelVersion"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#**Models training**\n\n---\n\nThis notebook is dedicated to training deep learning models for the task of **aortic segmentation**. To address this challenge, we selected five different segmentation models for comparison:\n1. **UNet**\n2. **FCN with ResNet-50 backbone**\n3. **FCN with pretrained ResNet-50 backbone**\n4. **DeepLabV3 with MobileNetV3-Large backbone**\n5. **DeepLabV3 with pretrained MobileNetV3-Large backbone**\n\nThe notebook includes:\n* Loading and preprocessing of the dataset  \n* Definition of the training class (`TrainModel`)  \n* Hyperparameter optimization using **Optuna**\n\nDuring optimization, the **best-performing models from each trial loop are saved** and will later be **used for evaluation and comparison of final results**.","metadata":{}},{"cell_type":"code","source":"!pip install monai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T09:34:45.614757Z","iopub.execute_input":"2025-08-05T09:34:45.615211Z","iopub.status.idle":"2025-08-05T09:36:05.855508Z","shell.execute_reply.started":"2025-08-05T09:34:45.615188Z","shell.execute_reply":"2025-08-05T09:36:05.854626Z"}},"outputs":[{"name":"stdout","text":"Collecting monai\n  Downloading monai-1.5.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.11/dist-packages (from monai) (1.26.4)\nRequirement already satisfied: torch<2.7.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from monai) (2.6.0+cu124)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.24->monai) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.7.0,>=2.4.1->monai) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.7.0,>=2.4.1->monai) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.24->monai) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.24->monai) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.24->monai) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.24->monai) (2024.2.0)\nDownloading monai-1.5.0-py3-none-any.whl (2.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, monai\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed monai-1.5.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport json\nfrom sklearn.model_selection import train_test_split\nimport optuna\nimport torch as tc\nimport torch.nn as nn\nimport json\nimport time\nfrom torchvision import models\nfrom torchvision.models.segmentation import FCN_ResNet50_Weights, DeepLabV3_MobileNet_V3_Large_Weights\nfrom monai.networks.nets import UNet\nfrom monai.losses import DiceLoss\n\n\nimport sys\nsys.path.insert(1, \"/kaggle/input/d/wiktorkilian/dataset-loader/\")\nfrom Dataset_loader import Dataset","metadata":{"id":"bzjMYvclP33s","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T09:36:49.675114Z","iopub.execute_input":"2025-08-05T09:36:49.675980Z","iopub.status.idle":"2025-08-05T09:36:49.680992Z","shell.execute_reply.started":"2025-08-05T09:36:49.675933Z","shell.execute_reply":"2025-08-05T09:36:49.680380Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def get_files(data_path, folder):\n    names = []\n    folder_path = os.path.join(data_path, folder)\n    for subfolder in os.listdir(folder_path):\n        subfolder_path = os.path.join(folder_path, subfolder)\n        for file in os.listdir(subfolder_path):\n            if file.endswith(\".nrrd\") and not file.endswith(\".seg.nrrd\"):\n                names.append(os.path.join(subfolder_path, file))\n    return names\n\ndef split_data(names_list, pre_test_size = 0.2, val_ratio = 0.5, seed=5):\n    train_names, pre_test_names = train_test_split(names_list, test_size = pre_test_size, random_state = seed)\n    val_names, test_names = train_test_split(pre_test_names, test_size = val_ratio, random_state = seed)\n    return train_names, val_names, test_names\n\nData_path = r\"/kaggle/input/mri-data/Data\"\n\nDongyang_data_names = get_files(Data_path, \"Dongyang\")\nKiTS_data_names = get_files(Data_path, \"KiTS\")\nRider_data_names = get_files(Data_path, \"Rider\")\n\nD_train_names, D_val_names, D_test_names = split_data(Dongyang_data_names)\nK_train_names, K_val_names, K_test_names = split_data(KiTS_data_names)\nR_train_names, R_val_names, R_test_names = split_data(Rider_data_names)\n\ntrain_data_names = D_train_names + K_train_names + R_train_names\nval_data_names = D_val_names + K_val_names + R_val_names\ntest_data_names = D_test_names + K_test_names + R_test_names\n\ndata_names_dict = {\n    \"train\": train_data_names,\n    \"validation\": val_data_names,\n    \"test\": test_data_names\n}\n\nwith open(\"/kaggle/working/data_names_dict.txt\", \"w\") as file:\n    json.dump(data_names_dict, file, indent=2)","metadata":{"id":"FKovLCV6fqX8","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T09:36:49.682206Z","iopub.execute_input":"2025-08-05T09:36:49.682416Z","iopub.status.idle":"2025-08-05T09:36:50.412245Z","shell.execute_reply.started":"2025-08-05T09:36:49.682399Z","shell.execute_reply":"2025-08-05T09:36:50.411715Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"Training_dataset = Dataset(train_data_names)\nTraining_dataset.preprocess_data()\n\nValidation_dataset = Dataset(val_data_names)\nValidation_dataset.preprocess_data()","metadata":{"id":"vaDKpghXkvOA","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T09:36:50.412955Z","iopub.execute_input":"2025-08-05T09:36:50.413209Z","iopub.status.idle":"2025-08-05T09:41:13.958158Z","shell.execute_reply.started":"2025-08-05T09:36:50.413184Z","shell.execute_reply":"2025-08-05T09:41:13.957562Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class TrainModel:\n    def __init__(self, model, training_loader, validation_loader, learning_rate, num_epochs, early_stopping = True, mode = \"Study\"):\n        self.model = model\n        self.training_loader = training_loader\n        self.validation_loader = validation_loader\n        self.learning_rate = learning_rate\n        self.num_epochs = num_epochs\n        self.early_stopping = early_stopping \n        self.mode = mode\n        \n        self.losses = []\n        self.val_losses = []\n        self.best_val_loss = float(\"inf\")\n        self.patience = 0\n        self.patience_limit = 3\n\n        self.obj_func = DiceLoss(sigmoid=True)\n        self.optimizer = tc.optim.Adam(model.parameters(), lr=learning_rate)\n        self.device = tc.device(\"cuda\" if tc.cuda.is_available() else \"cpu\")\n\n    def training_loop(self, validation=False):\n        epoch_loss = 0\n        loader = self.validation_loader if validation else self.training_loader\n\n        for images, masks in loader:\n            images, masks = images.to(self.device), masks.to(self.device)\n            output = self.model(images.unsqueeze(1))\n            if isinstance(output, dict):\n                output = output[\"out\"]\n            loss = self.obj_func(output, masks.unsqueeze(1))\n            if not validation:\n                loss.backward()\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n            epoch_loss += loss.item() * images.size(0)\n\n        self.val_losses.append(epoch_loss/len(loader.dataset)) if validation else self.losses.append(epoch_loss/len(loader.dataset))\n    \n    def check_early_stopping(self, val_loss):\n        if val_loss < self.best_val_loss:\n            self.best_val_loss = val_loss\n            self.patience = 0\n        else:\n            self.patience += 1\n        if self.patience >= self.patience_limit:\n            return True\n        return False\n    \n    def print_epoch_info(self, epoch):\n        print(f\"\\nCurrent epoch: {epoch+1}\")\n        print(f\"Train loss: {self.losses[-1]:.4f}\") \n        print(f\"Validation loss: {self.val_losses[-1]:.4f}\") \n\n    def train(self):\n        self.model = self.model.to(self.device)\n        if tc.cuda.device_count() > 1: #Kaggle offers 2xT4, we divide our computing power beetwen those two (DataParallel)\n            self.model = nn.DataParallel(self.model)\n        \n        for epoch in range(self.num_epochs):\n            self.model.train()\n            self.training_loop(validation=False)\n            self.model.eval()\n            with tc.no_grad():\n                self.training_loop(validation=True)\n            \n            self.print_epoch_info(epoch) if self.mode == \"Training\" else None\n            if self.early_stopping and self.check_early_stopping(val_loss=self.val_losses[-1]):\n                break\n\n    def get_losses(self):\n        return self.losses, self.val_losses\n    \n    def get_model(self):\n        return self.model","metadata":{"id":"O4BMZVNsiCqd","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T09:41:13.958704Z","iopub.execute_input":"2025-08-05T09:41:13.958912Z","iopub.status.idle":"2025-08-05T09:41:13.969549Z","shell.execute_reply.started":"2025-08-05T09:41:13.958895Z","shell.execute_reply":"2025-08-05T09:41:13.968798Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def logging_callback(study, trial):\n    print(f\"[Trial {trial.number}] Params: {trial.params}\")\n    if study.best_trial.number == trial.number:\n        print(f\"New best result\")\n\noptuna.logging.set_verbosity(optuna.logging.WARNING)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T09:41:13.970410Z","iopub.execute_input":"2025-08-05T09:41:13.970697Z","iopub.status.idle":"2025-08-05T09:41:13.992825Z","shell.execute_reply.started":"2025-08-05T09:41:13.970672Z","shell.execute_reply":"2025-08-05T09:41:13.992285Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def run_trial_training(model, training_loader, validation_loader, learning_rate, num_epochs, best_val_loss, model_name, trial):\n    start_time = time.time()\n    train_model = TrainModel(model, training_loader, validation_loader, learning_rate, num_epochs, early_stopping = True, mode = \"Study\")\n    train_model.train()\n    final_losses, final_val_losses = train_model.get_losses()\n    end_time =  time.time()\n    \n    if final_val_losses[-1] < best_val_loss:\n        best_val_loss = final_val_losses[-1]\n        training_info = {'train_loss_lst': final_losses, 'val_loss_lst': final_val_losses, 'time': end_time-start_time}\n        model = train_model.get_model()\n        tc.save(training_info, f\"/kaggle/working/{model_name}_info.pt\")\n        tc.save(model.state_dict(), f\"/kaggle/working/{model_name}_trained.pth\")\n\n    best_epoch = final_val_losses.index(min(final_val_losses)) + 1\n    training_time = time.strftime(\"%H:%M:%S\", time.gmtime(end_time-start_time))\n    print(f\"\\n--- Trial {trial.number} ---\")\n    print(f\"Final val loss: {final_val_losses[-1]:.4f}\")\n    print(f\"Best epoch: {best_epoch} - loss value: {min(final_val_losses)}\")\n    print(f\"Training time: {training_time}\")\n    \n    return final_val_losses[-1], best_val_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T09:41:13.993525Z","iopub.execute_input":"2025-08-05T09:41:13.993767Z","iopub.status.idle":"2025-08-05T09:41:14.010036Z","shell.execute_reply.started":"2025-08-05T09:41:13.993746Z","shell.execute_reply":"2025-08-05T09:41:14.009506Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"best_val_loss = float(\"inf\")\n\ndef objective(trial):\n    global best_val_loss\n    channels = trial.suggest_categorical(\"channels\", [[16, 32, 64, 128], [16, 32, 64, 128, 256]])\n    dropout = trial.suggest_float(\"dropout\", 0.0, 0.2)\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n    num_epochs = trial .suggest_int(\"num_epochs\", 2, 15)\n    \n\n    training_loader = tc.utils.data.DataLoader(Training_dataset, batch_size=32, shuffle=True)\n    validation_loader = tc.utils.data.DataLoader(Validation_dataset, batch_size=32, shuffle=True)\n\n    strides = (2,) * (len(channels) - 1)\n    model = UNet(\n        spatial_dims = 2,\n        in_channels=1,\n        out_channels=1,\n        channels=channels,\n        strides=strides,\n        dropout=dropout\n    )\n\n    last_loss, best_val_loss = run_trial_training(model, training_loader, validation_loader, learning_rate, num_epochs, best_val_loss, model_name=\"UNet\", trial=trial)\n    \n    tc.cuda.empty_cache()\n    return last_loss\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=20, callbacks=[logging_callback])\n\nprint(f\"\\nBest trial: #{study.best_trial.number}\")\nprint(f\"  Value (val_loss): {study.best_trial.value:.4f}\")\nprint(f\"  Params: {study.best_trial.params}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PDAEeUUokzwY","outputId":"3c479369-4167-44b1-a4c0-908499b71749","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T00:26:03.250078Z","iopub.execute_input":"2025-08-05T00:26:03.250344Z","iopub.status.idle":"2025-08-05T01:08:49.824479Z","shell.execute_reply.started":"2025-08-05T00:26:03.250324Z","shell.execute_reply":"2025-08-05T01:08:49.823799Z"}},"outputs":[{"name":"stdout","text":"\n--- Trial 0 ---\nFinal val loss: 0.3488\nBest epoch: 3 - loss value: 0.3360832684304808\nTraining time: 00:01:41\n[Trial 0] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.01077287031055012, 'learning_rate': 0.00866984140875161, 'num_epochs': 12}\nNew best result\n\n--- Trial 1 ---\nFinal val loss: 0.3331\nBest epoch: 9 - loss value: 0.32696925308567387\nTraining time: 00:02:52\n[Trial 1] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.11292249111318349, 'learning_rate': 0.0011799895057967428, 'num_epochs': 12}\nNew best result\n\n--- Trial 2 ---\nFinal val loss: 0.3250\nBest epoch: 9 - loss value: 0.32497288228004806\nTraining time: 00:02:09\n[Trial 2] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.01754145086034069, 'learning_rate': 0.009957769564287594, 'num_epochs': 9}\nNew best result\n\n--- Trial 3 ---\nFinal val loss: 0.3921\nBest epoch: 2 - loss value: 0.3710207793695685\nTraining time: 00:00:43\n[Trial 3] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.0150118858947073, 'learning_rate': 0.008145416714722334, 'num_epochs': 3}\n\n--- Trial 4 ---\nFinal val loss: 0.3406\nBest epoch: 4 - loss value: 0.3405796685974547\nTraining time: 00:00:57\n[Trial 4] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.0815430268339492, 'learning_rate': 0.004399306309432153, 'num_epochs': 4}\n\n--- Trial 5 ---\nFinal val loss: 0.2927\nBest epoch: 9 - loss value: 0.2926879554236605\nTraining time: 00:02:31\n[Trial 5] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.16454162080022391, 'learning_rate': 0.007402390161936459, 'num_epochs': 9}\nNew best result\n\n--- Trial 6 ---\nFinal val loss: 0.3387\nBest epoch: 3 - loss value: 0.33868337795378856\nTraining time: 00:00:49\n[Trial 6] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.16143198689057014, 'learning_rate': 0.0005405417245781457, 'num_epochs': 3}\n\n--- Trial 7 ---\nFinal val loss: 0.3476\nBest epoch: 10 - loss value: 0.3475699373373607\nTraining time: 00:02:45\n[Trial 7] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.19281047365691115, 'learning_rate': 0.00017959710485015146, 'num_epochs': 10}\n\n--- Trial 8 ---\nFinal val loss: 0.3405\nBest epoch: 3 - loss value: 0.3404970100189194\nTraining time: 00:00:49\n[Trial 8] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.16860529292740076, 'learning_rate': 0.0007975335153717118, 'num_epochs': 3}\n\n--- Trial 9 ---\nFinal val loss: 0.4202\nBest epoch: 10 - loss value: 0.4201548841786123\nTraining time: 00:02:22\n[Trial 9] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.17848921235359855, 'learning_rate': 0.00014706585909883923, 'num_epochs': 10}\n\n--- Trial 10 ---\nFinal val loss: 0.9751\nBest epoch: 6 - loss value: 0.9750530486956498\nTraining time: 00:01:39\n[Trial 10] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.12621830324885816, 'learning_rate': 1.2023635255043611e-05, 'num_epochs': 6}\n\n--- Trial 11 ---\nFinal val loss: 0.3042\nBest epoch: 14 - loss value: 0.29901993507489305\nTraining time: 00:03:38\n[Trial 11] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.05635041218038816, 'learning_rate': 0.0023275175023659105, 'num_epochs': 15}\n\n--- Trial 12 ---\nFinal val loss: 0.3245\nBest epoch: 6 - loss value: 0.3075619201224569\nTraining time: 00:02:29\n[Trial 12] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.06299198930885, 'learning_rate': 0.0020600177630216016, 'num_epochs': 15}\n\n--- Trial 13 ---\nFinal val loss: 0.3003\nBest epoch: 14 - loss value: 0.30030020415359654\nTraining time: 00:03:18\n[Trial 13] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.05582385693125196, 'learning_rate': 0.002621674093687962, 'num_epochs': 14}\n\n--- Trial 14 ---\nFinal val loss: 0.8754\nBest epoch: 7 - loss value: 0.8753685902748822\nTraining time: 00:01:55\n[Trial 14] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.13992674550396067, 'learning_rate': 4.438222016820365e-05, 'num_epochs': 7}\n\n--- Trial 15 ---\nFinal val loss: 0.3125\nBest epoch: 9 - loss value: 0.31005924361955745\nTraining time: 00:02:50\n[Trial 15] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.08814836799137725, 'learning_rate': 0.002862504980422368, 'num_epochs': 13}\n\n--- Trial 16 ---\nFinal val loss: 0.3723\nBest epoch: 6 - loss value: 0.3723269674260821\nTraining time: 00:01:25\n[Trial 16] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.03991845226486533, 'learning_rate': 0.00036957440403279947, 'num_epochs': 6}\n\n--- Trial 17 ---\nFinal val loss: 0.3035\nBest epoch: 8 - loss value: 0.2981284043148612\nTraining time: 00:03:00\n[Trial 17] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.1406618278167201, 'learning_rate': 0.001188481748489466, 'num_epochs': 13}\n\n--- Trial 18 ---\nFinal val loss: 0.3051\nBest epoch: 5 - loss value: 0.2988763092947899\nTraining time: 00:02:10\n[Trial 18] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.10320042194354395, 'learning_rate': 0.0046332963832921275, 'num_epochs': 8}\n\n--- Trial 19 ---\nFinal val loss: 0.3168\nBest epoch: 11 - loss value: 0.316846629856016\nTraining time: 00:02:35\n[Trial 19] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.04111111370383023, 'learning_rate': 0.004507851011153447, 'num_epochs': 11}\n\nBest trial: #5\n  Value (val_loss): 0.2927\n  Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.16454162080022391, 'learning_rate': 0.007402390161936459, 'num_epochs': 9}\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"best_val_loss = float(\"inf\")\n\ndef pytorch_objective_FCN(trial):\n    global best_val_loss\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n    num_epochs = trial .suggest_int(\"num_epochs\", 2, 15)\n\n    training_loader = tc.utils.data.DataLoader(Training_dataset, batch_size=64, shuffle=True)\n    validation_loader = tc.utils.data.DataLoader(Validation_dataset, batch_size=64, shuffle=True)\n\n    model = models.segmentation.fcn_resnet50(weights=None, num_classes=1)\n    model.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n    last_loss, best_val_loss = run_trial_training(model, training_loader, validation_loader, learning_rate, num_epochs, best_val_loss, model_name=\"FCN\", trial=trial)\n    \n    tc.cuda.empty_cache()\n    return last_loss\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(pytorch_objective_FCN, n_trials=5, callbacks=[logging_callback])\n\nprint(f\"\\nBest trial: #{study.best_trial.number}\")\nprint(f\"  Value (val_loss): {study.best_trial.value:.4f}\")\nprint(f\"  Params: {study.best_trial.params}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F7YL3EvYk1qC","outputId":"bd7e7e44-765a-4357-d1f1-8f0a3bd5f110","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T01:08:49.825498Z","iopub.execute_input":"2025-08-05T01:08:49.825771Z","iopub.status.idle":"2025-08-05T06:08:12.731185Z","shell.execute_reply.started":"2025-08-05T01:08:49.825752Z","shell.execute_reply":"2025-08-05T06:08:12.730320Z"}},"outputs":[{"name":"stdout","text":"\n--- Trial 0 ---\nFinal val loss: 0.3838\nBest epoch: 8 - loss value: 0.3838325616157463\nTraining time: 00:52:30\n[Trial 0] Params: {'learning_rate': 4.7326111416923234e-05, 'num_epochs': 8}\nNew best result\n\n--- Trial 1 ---\nFinal val loss: 0.3375\nBest epoch: 6 - loss value: 0.31712396952742683\nTraining time: 00:58:37\n[Trial 1] Params: {'learning_rate': 0.0004635054719612116, 'num_epochs': 10}\nNew best result\n\n--- Trial 2 ---\nFinal val loss: 0.5405\nBest epoch: 14 - loss value: 0.5404831943318027\nTraining time: 01:32:21\n[Trial 2] Params: {'learning_rate': 1.4579548300895439e-05, 'num_epochs': 14}\n\n--- Trial 3 ---\nFinal val loss: 0.6268\nBest epoch: 5 - loss value: 0.6267939262654876\nTraining time: 00:33:02\n[Trial 3] Params: {'learning_rate': 1.2871503088557745e-05, 'num_epochs': 5}\n\n--- Trial 4 ---\nFinal val loss: 0.3624\nBest epoch: 7 - loss value: 0.35577251570235535\nTraining time: 01:02:46\n[Trial 4] Params: {'learning_rate': 0.005876816821760395, 'num_epochs': 15}\n\nBest trial: #1\n  Value (val_loss): 0.3375\n  Params: {'learning_rate': 0.0004635054719612116, 'num_epochs': 10}\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"best_val_loss = float(\"inf\")\n\ndef pytorch_objective_FCN_pretrained(trial):\n    global best_val_loss\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n    num_epochs = trial .suggest_int(\"num_epochs\", 2, 15)\n\n    training_loader = tc.utils.data.DataLoader(Training_dataset, batch_size=64, shuffle=True)\n    validation_loader = tc.utils.data.DataLoader(Validation_dataset, batch_size=64, shuffle=True)\n\n    model = models.segmentation.fcn_resnet50(weights=FCN_ResNet50_Weights)\n    model.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    model.classifier[4] = nn.Conv2d(512, 1, kernel_size=1)\n\n    last_loss, best_val_loss = run_trial_training(model, training_loader, validation_loader, learning_rate, num_epochs, best_val_loss, model_name=\"FCN_pretrained\", trial=trial)\n    \n    tc.cuda.empty_cache()\n    return last_loss\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(pytorch_objective_FCN_pretrained, n_trials=5, callbacks=[logging_callback])\n\nprint(f\"\\nBest trial: #{study.best_trial.number}\")\nprint(f\"  Value (val_loss): {study.best_trial.value:.4f}\")\nprint(f\"  Params: {study.best_trial.params}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wc2yM4RKk9FH","outputId":"013a686c-312b-429c-eb12-0b058403156d","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T06:08:12.732124Z","iopub.execute_input":"2025-08-05T06:08:12.732746Z","iopub.status.idle":"2025-08-05T09:22:05.508900Z","shell.execute_reply.started":"2025-08-05T06:08:12.732716Z","shell.execute_reply":"2025-08-05T09:22:05.508207Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=FCN_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/fcn_resnet50_coco-1167a1af.pth\" to /root/.cache/torch/hub/checkpoints/fcn_resnet50_coco-1167a1af.pth\n100%|██████████| 135M/135M [00:00<00:00, 186MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"\n--- Trial 0 ---\nFinal val loss: 0.3513\nBest epoch: 7 - loss value: 0.35128252509334457\nTraining time: 00:46:35\n[Trial 0] Params: {'learning_rate': 0.00012397419410209337, 'num_epochs': 7}\nNew best result\n\n--- Trial 1 ---\nFinal val loss: 0.4337\nBest epoch: 9 - loss value: 0.4336885421256083\nTraining time: 01:00:24\n[Trial 1] Params: {'learning_rate': 4.7121657525888465e-05, 'num_epochs': 9}\n\n--- Trial 2 ---\nFinal val loss: 0.4228\nBest epoch: 9 - loss value: 0.42280878056396276\nTraining time: 01:00:18\n[Trial 2] Params: {'learning_rate': 4.8698753287319715e-05, 'num_epochs': 9}\n\n--- Trial 3 ---\nFinal val loss: 0.3530\nBest epoch: 2 - loss value: 0.35302645229692997\nTraining time: 00:13:11\n[Trial 3] Params: {'learning_rate': 0.0006344993994652242, 'num_epochs': 2}\n\n--- Trial 4 ---\nFinal val loss: 0.4913\nBest epoch: 2 - loss value: 0.49129426308106117\nTraining time: 00:13:18\n[Trial 4] Params: {'learning_rate': 0.0002665698117907311, 'num_epochs': 2}\n\nBest trial: #0\n  Value (val_loss): 0.3513\n  Params: {'learning_rate': 0.00012397419410209337, 'num_epochs': 7}\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"best_val_loss = float(\"inf\")\n\ndef objective_deeplabv3(trial):\n    global best_val_loss\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n    num_epochs = trial .suggest_int(\"num_epochs\", 2, 20)\n\n    training_loader = tc.utils.data.DataLoader(Training_dataset, batch_size=64, shuffle=True)\n    validation_loader = tc.utils.data.DataLoader(Validation_dataset, batch_size=64, shuffle=True)\n\n    model = models.segmentation.deeplabv3_mobilenet_v3_large(weights=None, num_classes=1)\n    model.backbone._modules[\"0\"]._modules[\"0\"] = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)\n\n    last_loss, best_val_loss = run_trial_training(model, training_loader, validation_loader, learning_rate, num_epochs, best_val_loss, model_name=\"DeepLabv3\", trial=trial)\n    \n    tc.cuda.empty_cache()\n    return last_loss\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective_deeplabv3, n_trials=10, callbacks=[logging_callback])\n\nprint(f\"\\nBest trial: #{study.best_trial.number}\")\nprint(f\"  Value (val_loss): {study.best_trial.value:.4f}\")\nprint(f\"  Params: {study.best_trial.params}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BKvqgpCFk-h_","outputId":"3a119af2-2fd2-415e-d544-4c782d983788","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T13:01:09.472693Z","iopub.execute_input":"2025-08-05T13:01:09.472965Z","iopub.status.idle":"2025-08-05T14:39:54.496443Z","shell.execute_reply.started":"2025-08-05T13:01:09.472947Z","shell.execute_reply":"2025-08-05T14:39:54.495674Z"}},"outputs":[{"name":"stdout","text":"\n--- Trial 0 ---\nFinal val loss: 0.6195\nBest epoch: 3 - loss value: 0.6195443281440908\nTraining time: 00:03:27\n[Trial 0] Params: {'learning_rate': 0.00041286358842804703, 'num_epochs': 3}\nNew best result\n\n--- Trial 1 ---\nFinal val loss: 0.4825\nBest epoch: 10 - loss value: 0.48248552621911617\nTraining time: 00:11:28\n[Trial 1] Params: {'learning_rate': 0.0015652831479416713, 'num_epochs': 10}\nNew best result\n\n--- Trial 2 ---\nFinal val loss: 0.4845\nBest epoch: 7 - loss value: 0.4845328123589498\nTraining time: 00:08:01\n[Trial 2] Params: {'learning_rate': 0.0033826286582314125, 'num_epochs': 7}\n\n--- Trial 3 ---\nFinal val loss: 0.5356\nBest epoch: 6 - loss value: 0.5243412455179524\nTraining time: 00:10:18\n[Trial 3] Params: {'learning_rate': 0.003658942384153327, 'num_epochs': 11}\n\n--- Trial 4 ---\nFinal val loss: 0.6404\nBest epoch: 2 - loss value: 0.6404491218926447\nTraining time: 00:02:17\n[Trial 4] Params: {'learning_rate': 0.001206018114562143, 'num_epochs': 2}\n\n--- Trial 5 ---\nFinal val loss: 0.5511\nBest epoch: 12 - loss value: 0.5450881237441914\nTraining time: 00:14:56\n[Trial 5] Params: {'learning_rate': 0.00022309123441317086, 'num_epochs': 13}\n\n--- Trial 6 ---\nFinal val loss: 0.5750\nBest epoch: 2 - loss value: 0.5662209518576069\nTraining time: 00:03:26\n[Trial 6] Params: {'learning_rate': 0.002377805771401891, 'num_epochs': 3}\n\n--- Trial 7 ---\nFinal val loss: 0.5124\nBest epoch: 5 - loss value: 0.4805503056424906\nTraining time: 00:09:09\n[Trial 7] Params: {'learning_rate': 0.0032472903397017853, 'num_epochs': 15}\n\n--- Trial 8 ---\nFinal val loss: 0.5822\nBest epoch: 15 - loss value: 0.579304225603329\nTraining time: 00:20:40\n[Trial 8] Params: {'learning_rate': 0.00010526498670624812, 'num_epochs': 19}\n\n--- Trial 9 ---\nFinal val loss: 0.6250\nBest epoch: 12 - loss value: 0.6219508703329087\nTraining time: 00:14:55\n[Trial 9] Params: {'learning_rate': 7.523657197518934e-05, 'num_epochs': 13}\n\nBest trial: #1\n  Value (val_loss): 0.4825\n  Params: {'learning_rate': 0.0015652831479416713, 'num_epochs': 10}\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"best_val_loss = float(\"inf\")\n\ndef objective_deeplabv3_pretrained(trial):\n    global best_val_loss\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n    num_epochs = trial .suggest_int(\"num_epochs\", 2, 20)\n\n    training_loader = tc.utils.data.DataLoader(Training_dataset, batch_size=64, shuffle=True)\n    validation_loader = tc.utils.data.DataLoader(Validation_dataset, batch_size=64, shuffle=True)\n\n    model = models.segmentation.deeplabv3_mobilenet_v3_large(weights=DeepLabV3_MobileNet_V3_Large_Weights)\n    model.backbone._modules[\"0\"]._modules[\"0\"] = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)\n    model.classifier[4] = nn.Conv2d(256, 1, kernel_size=1)\n\n    last_loss, best_val_loss = run_trial_training(model, training_loader, validation_loader, learning_rate, num_epochs, best_val_loss, model_name=\"DeepLabv3_pretrained\", trial=trial)\n    \n    tc.cuda.empty_cache()\n    return last_loss\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective_deeplabv3_pretrained, n_trials=10, callbacks=[logging_callback])\n\nprint(f\"\\nBest trial: #{study.best_trial.number}\")\nprint(f\"  Value (val_loss): {study.best_trial.value:.4f}\")\nprint(f\"  Params: {study.best_trial.params}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iWlk7aZulAWQ","outputId":"aeb52707-2eef-4d94-9c2c-bbc1bc081218","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T11:15:20.581169Z","iopub.execute_input":"2025-08-05T11:15:20.581485Z","iopub.status.idle":"2025-08-05T12:58:58.357575Z","shell.execute_reply.started":"2025-08-05T11:15:20.581467Z","shell.execute_reply":"2025-08-05T12:58:58.356895Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/deeplabv3_mobilenet_v3_large-fc3c493d.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_mobilenet_v3_large-fc3c493d.pth\n100%|██████████| 42.3M/42.3M [00:00<00:00, 163MB/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Trial 0 ---\nFinal val loss: 0.5207\nBest epoch: 9 - loss value: 0.5207283987469485\nTraining time: 00:10:58\n[Trial 0] Params: {'learning_rate': 0.00032718360242898033, 'num_epochs': 9}\nNew best result\n\n--- Trial 1 ---\nFinal val loss: 0.4534\nBest epoch: 12 - loss value: 0.4534124423797243\nTraining time: 00:14:38\n[Trial 1] Params: {'learning_rate': 0.0011963329885838906, 'num_epochs': 12}\nNew best result\n\n--- Trial 2 ---\nFinal val loss: 0.6264\nBest epoch: 19 - loss value: 0.6263532185646855\nTraining time: 00:23:10\n[Trial 2] Params: {'learning_rate': 1.773118724942854e-05, 'num_epochs': 19}\n\n--- Trial 3 ---\nFinal val loss: 0.5445\nBest epoch: 6 - loss value: 0.5134639170494597\nTraining time: 00:08:30\n[Trial 3] Params: {'learning_rate': 0.0024503796227529624, 'num_epochs': 7}\n\n--- Trial 4 ---\nFinal val loss: 0.6070\nBest epoch: 2 - loss value: 0.6069670074443189\nTraining time: 00:02:25\n[Trial 4] Params: {'learning_rate': 0.0032835626906802824, 'num_epochs': 2}\n\n--- Trial 5 ---\nFinal val loss: 0.5453\nBest epoch: 3 - loss value: 0.545282449739221\nTraining time: 00:03:38\n[Trial 5] Params: {'learning_rate': 0.001972896992123928, 'num_epochs': 3}\n\n--- Trial 6 ---\nFinal val loss: 0.5509\nBest epoch: 13 - loss value: 0.5508680991236821\nTraining time: 00:15:50\n[Trial 6] Params: {'learning_rate': 0.00010987431152567353, 'num_epochs': 13}\n\n--- Trial 7 ---\nFinal val loss: 0.6184\nBest epoch: 6 - loss value: 0.6183988105642788\nTraining time: 00:07:18\n[Trial 7] Params: {'learning_rate': 7.728392915600729e-05, 'num_epochs': 6}\n\n--- Trial 8 ---\nFinal val loss: 0.9155\nBest epoch: 2 - loss value: 0.8459427899664799\nTraining time: 00:03:39\n[Trial 8] Params: {'learning_rate': 0.0029554081162548804, 'num_epochs': 3}\n\n--- Trial 9 ---\nFinal val loss: 0.4842\nBest epoch: 11 - loss value: 0.4842371930992934\nTraining time: 00:13:23\n[Trial 9] Params: {'learning_rate': 0.00027707565475644746, 'num_epochs': 11}\n\nBest trial: #1\n  Value (val_loss): 0.4534\n  Params: {'learning_rate': 0.0011963329885838906, 'num_epochs': 12}\n","output_type":"stream"}],"execution_count":14}]}