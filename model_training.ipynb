{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Models training**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is dedicated to training deep learning models for the task of **aortic segmentation**. To address this challenge, we selected five different segmentation models for comparison:\n",
    "1. **UNet**\n",
    "2. **FCN with ResNet-50 backbone**\n",
    "3. **FCN with pretrained ResNet-50 backbone**\n",
    "4. **DeepLabV3 with MobileNetV3-Large backbone**\n",
    "5. **DeepLabV3 with pretrained MobileNetV3-Large backbone**\n",
    "\n",
    "The notebook includes:\n",
    "* Loading and preprocessing of the dataset  \n",
    "* Definition of the training class (`TrainModel`)  \n",
    "* Hyperparameter optimization using **Optuna**\n",
    "\n",
    "During optimization, the **best-performing models from each trial loop are saved** and will later be **used for evaluation and comparison of final results**.\n",
    "\n",
    "**Note:** The notebook has been launched on the Kaggle platform - it is worth remembering to install the Monai library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Importing Libraries and Modules**\n",
    " \n",
    "---\n",
    "\n",
    "The first step includes importing all necessary libraries and helper modules:\n",
    "\n",
    "- `os` – for navigating the dataset folder structure\n",
    "- `json` – for saving and loading file names in JSON format (e.g., for dataset splits)\n",
    "- `train_test_split` from `sklearn.model_selection` – to split the data into training and validation sets\n",
    "- `optuna` – for hyperparameter optimization\n",
    "- `torch`, `torch.nn` – core PyTorch libraries used for training models\n",
    "- `time` – for tracking training duration\n",
    "- `torchvision.models` – includes prebuilt segmentation models like FCN and DeepLabV3\n",
    "- `monai.networks.nets.UNet`, `monai.losses.DiceLoss` – UNet architecture and the Dice loss function used for the segmentation task\n",
    "- `sys` – used to add a custom dataset loader module (`Dataset_loader`) to the Python path (to se more look to Dataset_loader.py)\n",
    "\n",
    "From the custom module `Dataset_loader`, the `Dataset` class is imported, which handles loading and preprocessing of the aorta segmentation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T09:36:49.675980Z",
     "iopub.status.busy": "2025-08-05T09:36:49.675114Z",
     "iopub.status.idle": "2025-08-05T09:36:49.680992Z",
     "shell.execute_reply": "2025-08-05T09:36:49.680380Z",
     "shell.execute_reply.started": "2025-08-05T09:36:49.675933Z"
    },
    "id": "bzjMYvclP33s",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import torch as tc\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import time\n",
    "from torchvision import models\n",
    "from torchvision.models.segmentation import FCN_ResNet50_Weights, DeepLabV3_MobileNet_V3_Large_Weights\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import DiceLoss\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, \"/kaggle/input/dataset-loader/\") #Dataloader path\n",
    "from Dataset_loader import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset splitting and loading**\n",
    "\n",
    "---\n",
    "In this step, we prepare the dataset splits for model training, validation, and testing. Each of the three sub-datasets — **Dongyang**, **KiTS**, and **Rider** — is first processed using a helper function `get_files()` (as in the `preprocessing_testing` and `data_overview` notebooks), which collects paths to all raw `.nrrd` image files (excluding segmentation masks).\n",
    "\n",
    "After collecting all file paths, the `split_data()` helper function is used to divide each dataset into training, validation, and test sets. The split is done in two stages:\n",
    "* 80% of the data is kept for training, and 20% is set aside as \"pre-test\".\n",
    "* The \"pre-test\" is then split 50/50 into validation and test sets.\n",
    "\n",
    "A fixed random seed ensures that the splits remain consistent across different runs.\n",
    "\n",
    "Finally, the file paths for each subset (training, validation, and test) are combined across all three datasets and stored in a dictionary. This dictionary is saved to disk as a JSON file. In future notebooks, especially during model evaluation, we will load this saved file directly to ensure consistency and avoid regenerating the splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T09:36:49.682416Z",
     "iopub.status.busy": "2025-08-05T09:36:49.682206Z",
     "iopub.status.idle": "2025-08-05T09:36:50.412245Z",
     "shell.execute_reply": "2025-08-05T09:36:50.411715Z",
     "shell.execute_reply.started": "2025-08-05T09:36:49.682399Z"
    },
    "id": "FKovLCV6fqX8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_files(data_path, folder):\n",
    "    names = []\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    for subfolder in os.listdir(folder_path):\n",
    "        subfolder_path = os.path.join(folder_path, subfolder)\n",
    "        for file in os.listdir(subfolder_path):\n",
    "            if file.endswith(\".nrrd\") and not file.endswith(\".seg.nrrd\"):\n",
    "                names.append(os.path.join(subfolder_path, file))\n",
    "    return names\n",
    "\n",
    "def split_data(names_list, pre_test_size = 0.2, val_ratio = 0.5, seed=5):\n",
    "    train_names, pre_test_names = train_test_split(names_list, test_size = pre_test_size, random_state = seed)\n",
    "    val_names, test_names = train_test_split(pre_test_names, test_size = val_ratio, random_state = seed)\n",
    "    return train_names, val_names, test_names\n",
    "\n",
    "Data_path = r\"/kaggle/input/mri-data/Data\" #Dataset path\n",
    "\n",
    "Dongyang_data_names = get_files(Data_path, \"Dongyang\")\n",
    "KiTS_data_names = get_files(Data_path, \"KiTS\")\n",
    "Rider_data_names = get_files(Data_path, \"Rider\")\n",
    "\n",
    "D_train_names, D_val_names, D_test_names = split_data(Dongyang_data_names)\n",
    "K_train_names, K_val_names, K_test_names = split_data(KiTS_data_names)\n",
    "R_train_names, R_val_names, R_test_names = split_data(Rider_data_names)\n",
    "\n",
    "train_data_names = D_train_names + K_train_names + R_train_names\n",
    "val_data_names = D_val_names + K_val_names + R_val_names\n",
    "test_data_names = D_test_names + K_test_names + R_test_names\n",
    "\n",
    "data_names_dict = {\n",
    "    \"train\": train_data_names,\n",
    "    \"validation\": val_data_names,\n",
    "    \"test\": test_data_names\n",
    "}\n",
    "\n",
    "with open(\"/kaggle/working/data_names_dict.txt\", \"w\") as file:\n",
    "    json.dump(data_names_dict, file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the dataset, we use our custom `Dataset` class to load and preprocess the data. The class takes a list of file paths and handles the reading and formatting of image-mask pairs.\n",
    "\n",
    "We initialize two datasets:\n",
    "* `Training_dataset` – containing the training samples\n",
    "* `Validation_dataset` – used to monitor model performance during training\n",
    "\n",
    "The `.preprocess_data()` method is responsible for preprocessing the data – it loads the image and corresponding mask volumes, applies the necessary transformations, and prepares the data for training. For more details on the preprocessing steps, see the **`preprocessing_testing`** notebook and **`Dataset_loader.py`** file. At this stage, the data is ready for model training and hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T09:36:50.413209Z",
     "iopub.status.busy": "2025-08-05T09:36:50.412955Z",
     "iopub.status.idle": "2025-08-05T09:41:13.958158Z",
     "shell.execute_reply": "2025-08-05T09:41:13.957562Z",
     "shell.execute_reply.started": "2025-08-05T09:36:50.413184Z"
    },
    "id": "vaDKpghXkvOA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Training_dataset = Dataset(train_data_names)\n",
    "Training_dataset.preprocess_data()\n",
    "\n",
    "Validation_dataset = Dataset(val_data_names)\n",
    "Validation_dataset.preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Class Definition**\n",
    "\n",
    "---\n",
    "\n",
    "To modularize the training process and avoid code repetition, a custom `TrainModel` class was implemented. This class encapsulates the full training workflow, from loss computation and optimization to epoch-wise loss tracking and early stopping. The model itself is passed externally and not instantiated within the class.\n",
    "\n",
    "Key features of the class include:\n",
    "* **Separation of training and validation**: The `training_loop(validation=False)` method handles both training and validation. Its behavior changes depending on the `validation` flag, allowing shared logic while avoiding redundant code.\n",
    "* **Early stopping mechanism**: A basic early stopping strategy is included. Training is stopped if the validation loss does not improve for a specified number of consecutive epochs (default: 3), helping prevent overfitting.\n",
    "* **Loss tracking**: Both training and validation losses are recorded per epoch, making it easier to visualize learning dynamics or analyze performance trends.\n",
    "* **Device handling**: The class moves the model to the appropriate device (`cuda` if available), and supports multi-GPU training via `DataParallel` (when running on Kaggle).\n",
    "* **Model and loss access**: The trained model and recorded losses can be retrieved through `get_model()` and `get_losses()` respectively.\n",
    "* **Mode-based behavior**: The class supports configurable output depending on the mode (`\"Training\"` or `\"Study\"`), such as printing per-epoch logs only when desired.\n",
    "\n",
    "For the segmentation task, the **DiceLoss** function from the MONAI library was selected, as it performs well in medical image segmentation tasks. As the optimizer, **Adam** was used — a widely adopted choice known for its reliable performance and adaptive learning rate behavior across a variety of deep learning tasks.\n",
    "\n",
    "This object-oriented structure improves code organization and reuse throughout the notebook.\n",
    "\n",
    "**Note:** Some segmentation models from `torchvision`, such as **FCN** and **DeepLabV3**, return their output wrapped in a dictionary under the `\"out\"` key. Therefore, before computing the loss, we check if the model output is a dictionary:\n",
    "```python\n",
    "if isinstance(output, dict):\n",
    "    output = output[\"out\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T09:41:13.958912Z",
     "iopub.status.busy": "2025-08-05T09:41:13.958704Z",
     "iopub.status.idle": "2025-08-05T09:41:13.969549Z",
     "shell.execute_reply": "2025-08-05T09:41:13.968798Z",
     "shell.execute_reply.started": "2025-08-05T09:41:13.958895Z"
    },
    "id": "O4BMZVNsiCqd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TrainModel:\n",
    "    def __init__(self, model, training_loader, validation_loader, learning_rate, num_epochs, early_stopping = True, mode = \"Study\"):\n",
    "        self.model = model\n",
    "        self.training_loader = training_loader\n",
    "        self.validation_loader = validation_loader\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stopping = early_stopping \n",
    "        self.mode = mode\n",
    "        \n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.patience = 0\n",
    "        self.patience_limit = 3\n",
    "\n",
    "        self.obj_func = DiceLoss(sigmoid=True)\n",
    "        self.optimizer = tc.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        self.device = tc.device(\"cuda\" if tc.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def training_loop(self, validation=False):\n",
    "        epoch_loss = 0\n",
    "        loader = self.validation_loader if validation else self.training_loader\n",
    "\n",
    "        for images, masks in loader:\n",
    "            images, masks = images.to(self.device), masks.to(self.device)\n",
    "            output = self.model(images.unsqueeze(1))\n",
    "            if isinstance(output, dict):\n",
    "                output = output[\"out\"]\n",
    "            loss = self.obj_func(output, masks.unsqueeze(1))\n",
    "            if not validation:\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            epoch_loss += loss.item() * images.size(0)\n",
    "\n",
    "        self.val_losses.append(epoch_loss/len(loader.dataset)) if validation else self.losses.append(epoch_loss/len(loader.dataset))\n",
    "    \n",
    "    def check_early_stopping(self, val_loss):\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.patience = 0\n",
    "        else:\n",
    "            self.patience += 1\n",
    "        if self.patience >= self.patience_limit:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def print_epoch_info(self, epoch):\n",
    "        print(f\"\\nCurrent epoch: {epoch+1}\")\n",
    "        print(f\"Train loss: {self.losses[-1]:.4f}\") \n",
    "        print(f\"Validation loss: {self.val_losses[-1]:.4f}\") \n",
    "\n",
    "    def train(self):\n",
    "        self.model = self.model.to(self.device)\n",
    "        if tc.cuda.device_count() > 1: #Kaggle offers 2xT4, we divide our computing power beetwen those two (DataParallel)\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            self.training_loop(validation=False)\n",
    "            self.model.eval()\n",
    "            with tc.no_grad():\n",
    "                self.training_loop(validation=True)\n",
    "            \n",
    "            self.print_epoch_info(epoch) if self.mode == \"Training\" else None\n",
    "            if self.early_stopping and self.check_early_stopping(val_loss=self.val_losses[-1]):\n",
    "                break\n",
    "\n",
    "    def get_losses(self):\n",
    "        return self.losses, self.val_losses\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Optimization and Training**\n",
    "\n",
    "---\n",
    "\n",
    "We begin this section by implementing two utility functions essential for the Optuna-based hyperparameter tuning workflow:\n",
    "* `logging_callback()` - this function is responsible for printing Optuna trial-related messages. For each trial, it displays the tested hyperparameters, and if the current trial achieves the best validation result. This improves traceability and visibility of the tuning process, especially when running multiple trials.\n",
    "* `run_trial_training()` - The second function is shared across all model-specific objective() functions and encapsulates the core training routine for a single trial. It performs the following steps:\n",
    "    * initializes the TrainModel class, which handles the training loop and optional early stopping,\n",
    "    * starts the training process for the current model and hyperparameters,\n",
    "    * after training, compares the final validation loss with the best loss so far, if a new best is found saves the trained model weights (.pth file) and stores training information such as loss curves and training time (.pt file),\n",
    "    *  prints a brief summary of the trial,\n",
    "    *  returns the final validation loss from the training and the best validation loss achieved to the main Optuna optimization function.\n",
    "\n",
    "**Note:** The model saved is the one with the best final validation performance (after all epochs or early stopping) but we also print information about epoch with best validation loss to better understand training process.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T09:41:13.970697Z",
     "iopub.status.busy": "2025-08-05T09:41:13.970410Z",
     "iopub.status.idle": "2025-08-05T09:41:13.992825Z",
     "shell.execute_reply": "2025-08-05T09:41:13.992285Z",
     "shell.execute_reply.started": "2025-08-05T09:41:13.970672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def logging_callback(study, trial):\n",
    "    print(f\"[Trial {trial.number}] Params: {trial.params}\")\n",
    "    if study.best_trial.number == trial.number:\n",
    "        print(f\"New best result\")\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T09:41:13.993767Z",
     "iopub.status.busy": "2025-08-05T09:41:13.993525Z",
     "iopub.status.idle": "2025-08-05T09:41:14.010036Z",
     "shell.execute_reply": "2025-08-05T09:41:14.009506Z",
     "shell.execute_reply.started": "2025-08-05T09:41:13.993746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_trial_training(model, training_loader, validation_loader, learning_rate, num_epochs, best_val_loss, model_name, trial):\n",
    "    start_time = time.time()\n",
    "    train_model = TrainModel(model, training_loader, validation_loader, learning_rate, num_epochs, early_stopping = True, mode = \"Study\")\n",
    "    train_model.train()\n",
    "    final_losses, final_val_losses = train_model.get_losses()\n",
    "    end_time =  time.time()\n",
    "    \n",
    "    if final_val_losses[-1] < best_val_loss:\n",
    "        best_val_loss = final_val_losses[-1]\n",
    "        training_info = {'train_loss_lst': final_losses, 'val_loss_lst': final_val_losses, 'time': end_time-start_time}\n",
    "        model = train_model.get_model()\n",
    "        tc.save(training_info, f\"/kaggle/working/{model_name}_info.pt\")\n",
    "        tc.save(model.state_dict(), f\"/kaggle/working/{model_name}_trained.pth\")\n",
    "\n",
    "    best_epoch = final_val_losses.index(min(final_val_losses)) + 1\n",
    "    training_time = time.strftime(\"%H:%M:%S\", time.gmtime(end_time-start_time))\n",
    "    print(f\"\\n--- Trial {trial.number} ---\")\n",
    "    print(f\"Final val loss: {final_val_losses[-1]:.4f}\")\n",
    "    print(f\"Best epoch: {best_epoch} - loss value: {min(final_val_losses)}\")\n",
    "    print(f\"Training time: {training_time}\")\n",
    "    \n",
    "    return final_val_losses[-1], best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining utility functiins we perform hyperparameter optimization using **Optuna** for multiple segmentation models. Each model type (UNet, FCN, DeepLabV3) has a dedicated `objective()` function where we define the hyperparameters to be tuned, the model architecture, and the training procedure. The optimization is performed based on the **final validation loss** at the end of each training run. The model is saved only if its final validation loss is the best among all trials so far. This ensures that the saved model represents the one with the best overall performance at the end of training, which will later be used for evaluation on the test set (model_ebal.ipynb).\n",
    "\n",
    "The number of optimization trials for each model was selected based on their computational complexity:\n",
    "* **UNet**: 20 trials,\n",
    "* **FCN** (ResNet-50 backbone):\n",
    "    * 5 trials without pre-trained weights,\n",
    "    * 5 trials with pre-trained weights,\n",
    "* **DeepLabV3** (MobileNetV3-Large backbone):\n",
    "    * 10 trials without pre-trained weights,\n",
    "    * 10 trials with pre-trained weights.\n",
    "\n",
    "**Note:** Unlike U-Net, which was specifically designed for medical image segmentation, FCN and DeepLabV3 (with MobileNet or ResNet backbones) were originally developed for general-purpose semantic segmentation tasks. As a result, their architectures assume RGB input and multi-class outputs, which requires structural adjustments when applied to medical segmentation tasks — such as changing the first convolutional layer to accept single-channel input and modifying the classifier to output a binary mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-05T00:26:03.250344Z",
     "iopub.status.busy": "2025-08-05T00:26:03.250078Z",
     "iopub.status.idle": "2025-08-05T01:08:49.824479Z",
     "shell.execute_reply": "2025-08-05T01:08:49.823799Z",
     "shell.execute_reply.started": "2025-08-05T00:26:03.250324Z"
    },
    "id": "PDAEeUUokzwY",
    "outputId": "3c479369-4167-44b1-a4c0-908499b71749",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 0 ---\n",
      "Final val loss: 0.3488\n",
      "Best epoch: 3 - loss value: 0.3360832684304808\n",
      "Training time: 00:01:41\n",
      "[Trial 0] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.01077287031055012, 'learning_rate': 0.00866984140875161, 'num_epochs': 12}\n",
      "New best result\n",
      "\n",
      "--- Trial 1 ---\n",
      "Final val loss: 0.3331\n",
      "Best epoch: 9 - loss value: 0.32696925308567387\n",
      "Training time: 00:02:52\n",
      "[Trial 1] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.11292249111318349, 'learning_rate': 0.0011799895057967428, 'num_epochs': 12}\n",
      "New best result\n",
      "\n",
      "--- Trial 2 ---\n",
      "Final val loss: 0.3250\n",
      "Best epoch: 9 - loss value: 0.32497288228004806\n",
      "Training time: 00:02:09\n",
      "[Trial 2] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.01754145086034069, 'learning_rate': 0.009957769564287594, 'num_epochs': 9}\n",
      "New best result\n",
      "\n",
      "--- Trial 3 ---\n",
      "Final val loss: 0.3921\n",
      "Best epoch: 2 - loss value: 0.3710207793695685\n",
      "Training time: 00:00:43\n",
      "[Trial 3] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.0150118858947073, 'learning_rate': 0.008145416714722334, 'num_epochs': 3}\n",
      "\n",
      "--- Trial 4 ---\n",
      "Final val loss: 0.3406\n",
      "Best epoch: 4 - loss value: 0.3405796685974547\n",
      "Training time: 00:00:57\n",
      "[Trial 4] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.0815430268339492, 'learning_rate': 0.004399306309432153, 'num_epochs': 4}\n",
      "\n",
      "--- Trial 5 ---\n",
      "Final val loss: 0.2927\n",
      "Best epoch: 9 - loss value: 0.2926879554236605\n",
      "Training time: 00:02:31\n",
      "[Trial 5] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.16454162080022391, 'learning_rate': 0.007402390161936459, 'num_epochs': 9}\n",
      "New best result\n",
      "\n",
      "--- Trial 6 ---\n",
      "Final val loss: 0.3387\n",
      "Best epoch: 3 - loss value: 0.33868337795378856\n",
      "Training time: 00:00:49\n",
      "[Trial 6] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.16143198689057014, 'learning_rate': 0.0005405417245781457, 'num_epochs': 3}\n",
      "\n",
      "--- Trial 7 ---\n",
      "Final val loss: 0.3476\n",
      "Best epoch: 10 - loss value: 0.3475699373373607\n",
      "Training time: 00:02:45\n",
      "[Trial 7] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.19281047365691115, 'learning_rate': 0.00017959710485015146, 'num_epochs': 10}\n",
      "\n",
      "--- Trial 8 ---\n",
      "Final val loss: 0.3405\n",
      "Best epoch: 3 - loss value: 0.3404970100189194\n",
      "Training time: 00:00:49\n",
      "[Trial 8] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.16860529292740076, 'learning_rate': 0.0007975335153717118, 'num_epochs': 3}\n",
      "\n",
      "--- Trial 9 ---\n",
      "Final val loss: 0.4202\n",
      "Best epoch: 10 - loss value: 0.4201548841786123\n",
      "Training time: 00:02:22\n",
      "[Trial 9] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.17848921235359855, 'learning_rate': 0.00014706585909883923, 'num_epochs': 10}\n",
      "\n",
      "--- Trial 10 ---\n",
      "Final val loss: 0.9751\n",
      "Best epoch: 6 - loss value: 0.9750530486956498\n",
      "Training time: 00:01:39\n",
      "[Trial 10] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.12621830324885816, 'learning_rate': 1.2023635255043611e-05, 'num_epochs': 6}\n",
      "\n",
      "--- Trial 11 ---\n",
      "Final val loss: 0.3042\n",
      "Best epoch: 14 - loss value: 0.29901993507489305\n",
      "Training time: 00:03:38\n",
      "[Trial 11] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.05635041218038816, 'learning_rate': 0.0023275175023659105, 'num_epochs': 15}\n",
      "\n",
      "--- Trial 12 ---\n",
      "Final val loss: 0.3245\n",
      "Best epoch: 6 - loss value: 0.3075619201224569\n",
      "Training time: 00:02:29\n",
      "[Trial 12] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.06299198930885, 'learning_rate': 0.0020600177630216016, 'num_epochs': 15}\n",
      "\n",
      "--- Trial 13 ---\n",
      "Final val loss: 0.3003\n",
      "Best epoch: 14 - loss value: 0.30030020415359654\n",
      "Training time: 00:03:18\n",
      "[Trial 13] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.05582385693125196, 'learning_rate': 0.002621674093687962, 'num_epochs': 14}\n",
      "\n",
      "--- Trial 14 ---\n",
      "Final val loss: 0.8754\n",
      "Best epoch: 7 - loss value: 0.8753685902748822\n",
      "Training time: 00:01:55\n",
      "[Trial 14] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.13992674550396067, 'learning_rate': 4.438222016820365e-05, 'num_epochs': 7}\n",
      "\n",
      "--- Trial 15 ---\n",
      "Final val loss: 0.3125\n",
      "Best epoch: 9 - loss value: 0.31005924361955745\n",
      "Training time: 00:02:50\n",
      "[Trial 15] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.08814836799137725, 'learning_rate': 0.002862504980422368, 'num_epochs': 13}\n",
      "\n",
      "--- Trial 16 ---\n",
      "Final val loss: 0.3723\n",
      "Best epoch: 6 - loss value: 0.3723269674260821\n",
      "Training time: 00:01:25\n",
      "[Trial 16] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.03991845226486533, 'learning_rate': 0.00036957440403279947, 'num_epochs': 6}\n",
      "\n",
      "--- Trial 17 ---\n",
      "Final val loss: 0.3035\n",
      "Best epoch: 8 - loss value: 0.2981284043148612\n",
      "Training time: 00:03:00\n",
      "[Trial 17] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.1406618278167201, 'learning_rate': 0.001188481748489466, 'num_epochs': 13}\n",
      "\n",
      "--- Trial 18 ---\n",
      "Final val loss: 0.3051\n",
      "Best epoch: 5 - loss value: 0.2988763092947899\n",
      "Training time: 00:02:10\n",
      "[Trial 18] Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.10320042194354395, 'learning_rate': 0.0046332963832921275, 'num_epochs': 8}\n",
      "\n",
      "--- Trial 19 ---\n",
      "Final val loss: 0.3168\n",
      "Best epoch: 11 - loss value: 0.316846629856016\n",
      "Training time: 00:02:35\n",
      "[Trial 19] Params: {'channels': [16, 32, 64, 128], 'dropout': 0.04111111370383023, 'learning_rate': 0.004507851011153447, 'num_epochs': 11}\n",
      "\n",
      "Best trial: #5\n",
      "  Value (val_loss): 0.2927\n",
      "  Params: {'channels': [16, 32, 64, 128, 256], 'dropout': 0.16454162080022391, 'learning_rate': 0.007402390161936459, 'num_epochs': 9}\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "def objective(trial):\n",
    "    global best_val_loss\n",
    "    channels = trial.suggest_categorical(\"channels\", [[16, 32, 64, 128], [16, 32, 64, 128, 256]])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.2)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    num_epochs = trial .suggest_int(\"num_epochs\", 2, 15)\n",
    "    \n",
    "\n",
    "    training_loader = tc.utils.data.DataLoader(Training_dataset, batch_size=32, shuffle=True)\n",
    "    validation_loader = tc.utils.data.DataLoader(Validation_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    strides = (2,) * (len(channels) - 1)\n",
    "    model = UNet(\n",
    "        spatial_dims = 2,\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        channels=channels,\n",
    "        strides=strides,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    last_loss, best_val_loss = run_trial_training(model, training_loader, validation_loader, learning_rate, num_epochs, best_val_loss, model_name=\"UNet\", trial=trial)\n",
    "    \n",
    "    tc.cuda.empty_cache()\n",
    "    return last_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20, callbacks=[logging_callback])\n",
    "\n",
    "print(f\"\\nBest trial: #{study.best_trial.number}\")\n",
    "print(f\"  Value (val_loss): {study.best_trial.value:.4f}\")\n",
    "print(f\"  Params: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-05T01:08:49.825771Z",
     "iopub.status.busy": "2025-08-05T01:08:49.825498Z",
     "iopub.status.idle": "2025-08-05T06:08:12.731185Z",
     "shell.execute_reply": "2025-08-05T06:08:12.730320Z",
     "shell.execute_reply.started": "2025-08-05T01:08:49.825752Z"
    },
    "id": "F7YL3EvYk1qC",
    "outputId": "bd7e7e44-765a-4357-d1f1-8f0a3bd5f110",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 0 ---\n",
      "Final val loss: 0.3838\n",
      "Best epoch: 8 - loss value: 0.3838325616157463\n",
      "Training time: 00:52:30\n",
      "[Trial 0] Params: {'learning_rate': 4.7326111416923234e-05, 'num_epochs': 8}\n",
      "New best result\n",
      "\n",
      "--- Trial 1 ---\n",
      "Final val loss: 0.3375\n",
      "Best epoch: 6 - loss value: 0.31712396952742683\n",
      "Training time: 00:58:37\n",
      "[Trial 1] Params: {'learning_rate': 0.0004635054719612116, 'num_epochs': 10}\n",
      "New best result\n",
      "\n",
      "--- Trial 2 ---\n",
      "Final val loss: 0.5405\n",
      "Best epoch: 14 - loss value: 0.5404831943318027\n",
      "Training time: 01:32:21\n",
      "[Trial 2] Params: {'learning_rate': 1.4579548300895439e-05, 'num_epochs': 14}\n",
      "\n",
      "--- Trial 3 ---\n",
      "Final val loss: 0.6268\n",
      "Best epoch: 5 - loss value: 0.6267939262654876\n",
      "Training time: 00:33:02\n",
      "[Trial 3] Params: {'learning_rate': 1.2871503088557745e-05, 'num_epochs': 5}\n",
      "\n",
      "--- Trial 4 ---\n",
      "Final val loss: 0.3624\n",
      "Best epoch: 7 - loss value: 0.35577251570235535\n",
      "Training time: 01:02:46\n",
      "[Trial 4] Params: {'learning_rate': 0.005876816821760395, 'num_epochs': 15}\n",
      "\n",
      "Best trial: #1\n",
      "  Value (val_loss): 0.3375\n",
      "  Params: {'learning_rate': 0.0004635054719612116, 'num_epochs': 10}\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "def pytorch_objective_FCN(trial):\n",
    "    global best_val_loss\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    num_epochs = trial .suggest_int(\"num_epochs\", 2, 15)\n",
    "\n",
    "    training_loader = tc.utils.data.DataLoader(Training_dataset, batch_size=64, shuffle=True)\n",
    "    validation_loader = tc.utils.data.DataLoader(Validation_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    model = models.segmentation.fcn_resnet50(weights=None, num_classes=1)\n",
    "    model.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) #Adjusting the first convolutional layer to accept single-channel (grayscale) input\n",
    "\n",
    "    last_loss, best_val_loss = run_trial_training(model, training_loader, validation_loader, learning_rate, num_epochs, best_val_loss, model_name=\"FCN\", trial=trial)\n",
    "    \n",
    "    tc.cuda.empty_cache()\n",
    "    return last_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(pytorch_objective_FCN, n_trials=5, callbacks=[logging_callback])\n",
    "\n",
    "print(f\"\\nBest trial: #{study.best_trial.number}\")\n",
    "print(f\"  Value (val_loss): {study.best_trial.value:.4f}\")\n",
    "print(f\"  Params: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-05T06:08:12.732746Z",
     "iopub.status.busy": "2025-08-05T06:08:12.732124Z",
     "iopub.status.idle": "2025-08-05T09:22:05.508900Z",
     "shell.execute_reply": "2025-08-05T09:22:05.508207Z",
     "shell.execute_reply.started": "2025-08-05T06:08:12.732716Z"
    },
    "id": "wc2yM4RKk9FH",
    "outputId": "013a686c-312b-429c-eb12-0b058403156d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=FCN_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/fcn_resnet50_coco-1167a1af.pth\" to /root/.cache/torch/hub/checkpoints/fcn_resnet50_coco-1167a1af.pth\n",
      "100%|██████████| 135M/135M [00:00<00:00, 186MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 0 ---\n",
      "Final val loss: 0.3513\n",
      "Best epoch: 7 - loss value: 0.35128252509334457\n",
      "Training time: 00:46:35\n",
      "[Trial 0] Params: {'learning_rate': 0.00012397419410209337, 'num_epochs': 7}\n",
      "New best result\n",
      "\n",
      "--- Trial 1 ---\n",
      "Final val loss: 0.4337\n",
      "Best epoch: 9 - loss value: 0.4336885421256083\n",
      "Training time: 01:00:24\n",
      "[Trial 1] Params: {'learning_rate': 4.7121657525888465e-05, 'num_epochs': 9}\n",
      "\n",
      "--- Trial 2 ---\n",
      "Final val loss: 0.4228\n",
      "Best epoch: 9 - loss value: 0.42280878056396276\n",
      "Training time: 01:00:18\n",
      "[Trial 2] Params: {'learning_rate': 4.8698753287319715e-05, 'num_epochs': 9}\n",
      "\n",
      "--- Trial 3 ---\n",
      "Final val loss: 0.3530\n",
      "Best epoch: 2 - loss value: 0.35302645229692997\n",
      "Training time: 00:13:11\n",
      "[Trial 3] Params: {'learning_rate': 0.0006344993994652242, 'num_epochs': 2}\n",
      "\n",
      "--- Trial 4 ---\n",
      "Final val loss: 0.4913\n",
      "Best epoch: 2 - loss value: 0.49129426308106117\n",
      "Training time: 00:13:18\n",
      "[Trial 4] Params: {'learning_rate': 0.0002665698117907311, 'num_epochs': 2}\n",
      "\n",
      "Best trial: #0\n",
      "  Value (val_loss): 0.3513\n",
      "  Params: {'learning_rate': 0.00012397419410209337, 'num_epochs': 7}\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "def pytorch_objective_FCN_pretrained(trial):\n",
    "    global best_val_loss\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    num_epochs = trial .suggest_int(\"num_epochs\", 2, 15)\n",
    "\n",
    "    training_loader = tc.utils.data.DataLoader(Training_dataset, batch_size=64, shuffle=True)\n",
    "    validation_loader = tc.utils.data.DataLoader(Validation_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    model = models.segmentation.fcn_resnet50(weights=FCN_ResNet50_Weights)\n",
    "    model.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    model.classifier[4] = nn.Conv2d(512, 1, kernel_size=1) #Replace the final classifier layer to output a single class (binary segmentation)\n",
    "\n",
    "    last_loss, best_val_loss = run_trial_training(model, training_loader, validation_loader, learning_rate, num_epochs, best_val_loss, model_name=\"FCN_pretrained\", trial=trial)\n",
    "    \n",
    "    tc.cuda.empty_cache()\n",
    "    return last_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(pytorch_objective_FCN_pretrained, n_trials=5, callbacks=[logging_callback])\n",
    "\n",
    "print(f\"\\nBest trial: #{study.best_trial.number}\")\n",
    "print(f\"  Value (val_loss): {study.best_trial.value:.4f}\")\n",
    "print(f\"  Params: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-05T13:01:09.472965Z",
     "iopub.status.busy": "2025-08-05T13:01:09.472693Z",
     "iopub.status.idle": "2025-08-05T14:39:54.496443Z",
     "shell.execute_reply": "2025-08-05T14:39:54.495674Z",
     "shell.execute_reply.started": "2025-08-05T13:01:09.472947Z"
    },
    "id": "BKvqgpCFk-h_",
    "outputId": "3a119af2-2fd2-415e-d544-4c782d983788",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 0 ---\n",
      "Final val loss: 0.6195\n",
      "Best epoch: 3 - loss value: 0.6195443281440908\n",
      "Training time: 00:03:27\n",
      "[Trial 0] Params: {'learning_rate': 0.00041286358842804703, 'num_epochs': 3}\n",
      "New best result\n",
      "\n",
      "--- Trial 1 ---\n",
      "Final val loss: 0.4825\n",
      "Best epoch: 10 - loss value: 0.48248552621911617\n",
      "Training time: 00:11:28\n",
      "[Trial 1] Params: {'learning_rate': 0.0015652831479416713, 'num_epochs': 10}\n",
      "New best result\n",
      "\n",
      "--- Trial 2 ---\n",
      "Final val loss: 0.4845\n",
      "Best epoch: 7 - loss value: 0.4845328123589498\n",
      "Training time: 00:08:01\n",
      "[Trial 2] Params: {'learning_rate': 0.0033826286582314125, 'num_epochs': 7}\n",
      "\n",
      "--- Trial 3 ---\n",
      "Final val loss: 0.5356\n",
      "Best epoch: 6 - loss value: 0.5243412455179524\n",
      "Training time: 00:10:18\n",
      "[Trial 3] Params: {'learning_rate': 0.003658942384153327, 'num_epochs': 11}\n",
      "\n",
      "--- Trial 4 ---\n",
      "Final val loss: 0.6404\n",
      "Best epoch: 2 - loss value: 0.6404491218926447\n",
      "Training time: 00:02:17\n",
      "[Trial 4] Params: {'learning_rate': 0.001206018114562143, 'num_epochs': 2}\n",
      "\n",
      "--- Trial 5 ---\n",
      "Final val loss: 0.5511\n",
      "Best epoch: 12 - loss value: 0.5450881237441914\n",
      "Training time: 00:14:56\n",
      "[Trial 5] Params: {'learning_rate': 0.00022309123441317086, 'num_epochs': 13}\n",
      "\n",
      "--- Trial 6 ---\n",
      "Final val loss: 0.5750\n",
      "Best epoch: 2 - loss value: 0.5662209518576069\n",
      "Training time: 00:03:26\n",
      "[Trial 6] Params: {'learning_rate': 0.002377805771401891, 'num_epochs': 3}\n",
      "\n",
      "--- Trial 7 ---\n",
      "Final val loss: 0.5124\n",
      "Best epoch: 5 - loss value: 0.4805503056424906\n",
      "Training time: 00:09:09\n",
      "[Trial 7] Params: {'learning_rate': 0.0032472903397017853, 'num_epochs': 15}\n",
      "\n",
      "--- Trial 8 ---\n",
      "Final val loss: 0.5822\n",
      "Best epoch: 15 - loss value: 0.579304225603329\n",
      "Training time: 00:20:40\n",
      "[Trial 8] Params: {'learning_rate': 0.00010526498670624812, 'num_epochs': 19}\n",
      "\n",
      "--- Trial 9 ---\n",
      "Final val loss: 0.6250\n",
      "Best epoch: 12 - loss value: 0.6219508703329087\n",
      "Training time: 00:14:55\n",
      "[Trial 9] Params: {'learning_rate': 7.523657197518934e-05, 'num_epochs': 13}\n",
      "\n",
      "Best trial: #1\n",
      "  Value (val_loss): 0.4825\n",
      "  Params: {'learning_rate': 0.0015652831479416713, 'num_epochs': 10}\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "def objective_deeplabv3(trial):\n",
    "    global best_val_loss\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    num_epochs = trial .suggest_int(\"num_epochs\", 2, 20)\n",
    "\n",
    "    training_loader = tc.utils.data.DataLoader(Training_dataset, batch_size=64, shuffle=True)\n",
    "    validation_loader = tc.utils.data.DataLoader(Validation_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    model = models.segmentation.deeplabv3_mobilenet_v3_large(weights=None, num_classes=1)\n",
    "    model.backbone._modules[\"0\"]._modules[\"0\"] = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "    last_loss, best_val_loss = run_trial_training(model, training_loader, validation_loader, learning_rate, num_epochs, best_val_loss, model_name=\"DeepLabv3\", trial=trial)\n",
    "    \n",
    "    tc.cuda.empty_cache()\n",
    "    return last_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_deeplabv3, n_trials=10, callbacks=[logging_callback])\n",
    "\n",
    "print(f\"\\nBest trial: #{study.best_trial.number}\")\n",
    "print(f\"  Value (val_loss): {study.best_trial.value:.4f}\")\n",
    "print(f\"  Params: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-05T11:15:20.581485Z",
     "iopub.status.busy": "2025-08-05T11:15:20.581169Z",
     "iopub.status.idle": "2025-08-05T12:58:58.357575Z",
     "shell.execute_reply": "2025-08-05T12:58:58.356895Z",
     "shell.execute_reply.started": "2025-08-05T11:15:20.581467Z"
    },
    "id": "iWlk7aZulAWQ",
    "outputId": "aeb52707-2eef-4d94-9c2c-bbc1bc081218",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_mobilenet_v3_large-fc3c493d.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_mobilenet_v3_large-fc3c493d.pth\n",
      "100%|██████████| 42.3M/42.3M [00:00<00:00, 163MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 0 ---\n",
      "Final val loss: 0.5207\n",
      "Best epoch: 9 - loss value: 0.5207283987469485\n",
      "Training time: 00:10:58\n",
      "[Trial 0] Params: {'learning_rate': 0.00032718360242898033, 'num_epochs': 9}\n",
      "New best result\n",
      "\n",
      "--- Trial 1 ---\n",
      "Final val loss: 0.4534\n",
      "Best epoch: 12 - loss value: 0.4534124423797243\n",
      "Training time: 00:14:38\n",
      "[Trial 1] Params: {'learning_rate': 0.0011963329885838906, 'num_epochs': 12}\n",
      "New best result\n",
      "\n",
      "--- Trial 2 ---\n",
      "Final val loss: 0.6264\n",
      "Best epoch: 19 - loss value: 0.6263532185646855\n",
      "Training time: 00:23:10\n",
      "[Trial 2] Params: {'learning_rate': 1.773118724942854e-05, 'num_epochs': 19}\n",
      "\n",
      "--- Trial 3 ---\n",
      "Final val loss: 0.5445\n",
      "Best epoch: 6 - loss value: 0.5134639170494597\n",
      "Training time: 00:08:30\n",
      "[Trial 3] Params: {'learning_rate': 0.0024503796227529624, 'num_epochs': 7}\n",
      "\n",
      "--- Trial 4 ---\n",
      "Final val loss: 0.6070\n",
      "Best epoch: 2 - loss value: 0.6069670074443189\n",
      "Training time: 00:02:25\n",
      "[Trial 4] Params: {'learning_rate': 0.0032835626906802824, 'num_epochs': 2}\n",
      "\n",
      "--- Trial 5 ---\n",
      "Final val loss: 0.5453\n",
      "Best epoch: 3 - loss value: 0.545282449739221\n",
      "Training time: 00:03:38\n",
      "[Trial 5] Params: {'learning_rate': 0.001972896992123928, 'num_epochs': 3}\n",
      "\n",
      "--- Trial 6 ---\n",
      "Final val loss: 0.5509\n",
      "Best epoch: 13 - loss value: 0.5508680991236821\n",
      "Training time: 00:15:50\n",
      "[Trial 6] Params: {'learning_rate': 0.00010987431152567353, 'num_epochs': 13}\n",
      "\n",
      "--- Trial 7 ---\n",
      "Final val loss: 0.6184\n",
      "Best epoch: 6 - loss value: 0.6183988105642788\n",
      "Training time: 00:07:18\n",
      "[Trial 7] Params: {'learning_rate': 7.728392915600729e-05, 'num_epochs': 6}\n",
      "\n",
      "--- Trial 8 ---\n",
      "Final val loss: 0.9155\n",
      "Best epoch: 2 - loss value: 0.8459427899664799\n",
      "Training time: 00:03:39\n",
      "[Trial 8] Params: {'learning_rate': 0.0029554081162548804, 'num_epochs': 3}\n",
      "\n",
      "--- Trial 9 ---\n",
      "Final val loss: 0.4842\n",
      "Best epoch: 11 - loss value: 0.4842371930992934\n",
      "Training time: 00:13:23\n",
      "[Trial 9] Params: {'learning_rate': 0.00027707565475644746, 'num_epochs': 11}\n",
      "\n",
      "Best trial: #1\n",
      "  Value (val_loss): 0.4534\n",
      "  Params: {'learning_rate': 0.0011963329885838906, 'num_epochs': 12}\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "def objective_deeplabv3_pretrained(trial):\n",
    "    global best_val_loss\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    num_epochs = trial .suggest_int(\"num_epochs\", 2, 20)\n",
    "\n",
    "    training_loader = tc.utils.data.DataLoader(Training_dataset, batch_size=64, shuffle=True)\n",
    "    validation_loader = tc.utils.data.DataLoader(Validation_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    model = models.segmentation.deeplabv3_mobilenet_v3_large(weights=DeepLabV3_MobileNet_V3_Large_Weights)\n",
    "    model.backbone._modules[\"0\"]._modules[\"0\"] = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "    model.classifier[4] = nn.Conv2d(256, 1, kernel_size=1)\n",
    "\n",
    "    last_loss, best_val_loss = run_trial_training(model, training_loader, validation_loader, learning_rate, num_epochs, best_val_loss, model_name=\"DeepLabv3_pretrained\", trial=trial)\n",
    "    \n",
    "    tc.cuda.empty_cache()\n",
    "    return last_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_deeplabv3_pretrained, n_trials=10, callbacks=[logging_callback])\n",
    "\n",
    "print(f\"\\nBest trial: #{study.best_trial.number}\")\n",
    "print(f\"  Value (val_loss): {study.best_trial.value:.4f}\")\n",
    "print(f\"  Params: {study.best_trial.params}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6999385,
     "sourceId": 11209360,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8007097,
     "sourceId": 12670650,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 254146075,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
